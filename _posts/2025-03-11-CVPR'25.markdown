---
layout: post
title:  "看看 CVPR'25"
date:   2025-03-11 00:14:00 +0800
categories: posts
tag: autonomous driving
---

## DriveAlpha

第一个自动驾驶的类r1，被别人做了。但是还是一个比较简单的状态。

https://arxiv.org/abs/2503.07608

这篇论文我觉得做得很对的一点就是，结果是很容易用规则来判断对错的。它预测的是 acclerate, decelerate, keep, stop; straight, left, right 这些最高级的指令抽象（不过连变道都没有还是有点太抽象了），格式简单，planning 的指标也是简单的 F1 score。这点很对！RL 训练的时候要尝试那么多次，指标越简单越好。我正在绞尽脑汁想要把训练时用的指标搞简单，把需要 LLM 参与的不稳定的评价优化一下，他这个真是给我启发了。还有一个很可贵的点是 diversity 指标。因为自动驾驶并不存在一个最优解，所以鼓励模型去探索 gt 以外的其他解，这很正确。你做的对，你做得好啊。

顺带一提，他问问题的 prompt 和我构想的完全一致，要给 vlm 提供当前车辆的速度和高级指令，我还是想对了的，暗自庆幸一下。

然后就是我觉得不太好的地方。

一个是他的 CoT 其实很短，说是做了强 reasoning，其实感觉想得也不太多？他这个输出的长度都不用修改模型的 config，我觉得普通输出都能有这么长了。（不过 reasoning 的强弱也不能看链长短啦，想得更少就能出正确结果是天大的好事）

第二个是，他的输入是单张前视。首先，其他车辆速度是不可能知道的了。我曾经对 qwenvl 做过实验，用两张间隔 0.5s 的前视图，问他里面车的速度，基本能知道哪辆车动了哪辆车没动（远处的车效果差很多，一般都无脑判断成没动）。但是决策的时候，知道这些就很够啦，我就能决定要不要停车要不要绕行了。给一张图看图说话，你怎么知道车的速度的？其次，我是赞成环视的，因为开车的时候总要看后视镜吧，难道变道的时候就不看后车直接转向了？不过我现在也不打算在我第一个工作做环视，因为 VLM 现在对环视的理解是真的不敢恭维，我现在没有时间在这方面较劲，性价比比较低，以后解决吧。我现在觉得性价比最高的还是前视+历史图像。

第三个是，也不能说不好，只能说怀疑。他用的是 Qwen2VL-2B，模型非常小。那做出来的结果是不是可靠其实也要打一个问号，我是真的怀疑他的数据会不会情境太少产生过拟，难以服众啊。不过如果确实小模型就能有这样好的效果，那也是一个很好的消息。