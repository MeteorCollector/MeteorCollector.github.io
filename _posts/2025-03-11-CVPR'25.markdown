---
layout: post
title:  "看看 CVPR'25"
date:   2025-03-11 00:14:00 +0800
categories: posts
tag: autonomous driving
---

其实最近的论文都想扔在这里，不管是不是 cvpr25 的

## DriveAlpha (并非 CVPR'25)

第一个自动驾驶的类r1，被别人做了。但是还是一个比较简单的状态。

https://arxiv.org/abs/2503.07608

这篇论文我觉得做得很对的一点就是，结果是很容易用规则来判断对错的。它预测的是 acclerate, decelerate, keep, stop; straight, left, right 这些最高级的指令抽象（不过连变道都没有还是有点太抽象了），格式简单，planning 的指标也是简单的 F1 score。这点很对！RL 训练的时候要尝试那么多次，指标越简单越好。我正在绞尽脑汁想要把训练时用的指标搞简单，把需要 LLM 参与的不稳定的评价优化一下，他这个真是给我启发了。还有一个很可贵的点是 diversity 指标。因为自动驾驶并不存在一个最优解，所以鼓励模型去探索 gt 以外的其他解，这很正确。你做的对，你做得好啊。

顺带一提，他问问题的 prompt 和我构想的完全一致，要给 vlm 提供当前车辆的速度和高级指令，我还是想对了的，暗自庆幸一下。

然后就是我觉得不太好的地方。

一个是他的 CoT 其实很短，说是做了强 reasoning，其实感觉想得也不太多？他这个输出的长度都不用修改模型的 config，我觉得普通输出都能有这么长了。（不过 reasoning 的强弱也不能看链长短啦，想得更少就能出正确结果是天大的好事）

第二个是，他的输入是单张前视。首先，其他车辆速度是不可能知道的了。我曾经对 qwenvl 做过实验，用两张间隔 0.5s 的前视图，问他里面车的速度，基本能知道哪辆车动了哪辆车没动（远处的车效果差很多，一般都无脑判断成没动）。但是决策的时候，知道这些就很够啦，我就能决定要不要停车要不要绕行了。给一张图看图说话，你怎么知道车的速度的？其次，我是赞成环视的，因为开车的时候总要看后视镜吧，难道变道的时候就不看后车直接转向了？不过我现在也不打算在我第一个工作做环视，因为 VLM 现在对环视的理解是真的不敢恭维，我现在没有时间在这方面较劲，性价比比较低，以后解决吧。我现在觉得性价比最高的还是前视+历史图像。

第三个是，也不能说不好，只能说怀疑。他用的是 Qwen2VL-2B，模型非常小。那做出来的结果是不是可靠其实也要打一个问号，我是真的怀疑他的数据会不会情境太少产生过拟，难以服众啊。不过如果确实小模型就能有这样好的效果，那也是一个很好的消息。

## SimLingo

https://arxiv.org/pdf/2503.09594

点进去的一刻释怀了，原来是 CarLLaVA，现在改成这个名字了。相比之前论文多了一些东西，其中一个是 Action Dreaming，就是为了避免模型只根据画面做决策而忽略 Action，要刻意让模型做一些 ood 的与 export gt 不同的 Action 来让模型做预测，并判断这些 Action 能不能做。思路是在数据集里模拟，和我做我那个 Eval 的思路一样。

然后提到他们用 DriveLM-Carla 采数据的时候，会用 GPT 重写 20 个同义句，训练的时候随机 sample 来避免过拟合。我瞬间想到我这个是不是也要重写，然后想起来我好像已经要抛弃 sft 只关注正确性指标了。但是话又说回来，他只用 LoRA sft 和小模型 (InternVL2-1B, The InternVL2-1B model consists of the vision encoder InternViT-300M-448px (ViT) and Qwen2-
0.5B-Instruct as the language model (LLM)) 就做到了这些，也是很神奇了。我可以做到吗。不过也是一个好消息，把我换大模型（比如7B）的冲动打消了，应该是没什么必要。小模型好啊，负担得起，跑得起来。