---
layout: post
title:  "LLM4Drive 笔记：自动驾驶用大语言模型"
date:   2024-09-09 11:58:00 +0800
categories: posts
tag: autonomous driving
---

## 前情提要

这是一篇从 [LLM4Drive: A Survey of Large Language Models for Autonomous Driving](https://arxiv.org/abs/2311.01043) 伸展出去的笔记，主要看看自动驾驶用大语言模型。唉，大语言模型，[LMDrive](https://meteorcollector.github.io/2024/08/end-to-end-recent/#lmdrive-closed-loop-end-to-end-driving-with-large-language-models) 那篇文章对 LLM 的赞誉之词还历历在目。

> ... large language models (LLM) have shown impressive reasoning capabilities that approach "Artificial General Intelligence"...
>
> ...大语言模型在目前已经拥有了接近“通用人工智能”的惊艳表现...

看到如此正面的描述总是要从反面思考一下。在我看来，除了 LLM 模型结构本身适宜多模态数据处理（用不同的embedding当自然语言一样处理就可以了）之外，在比较抽象的认识层面上，LLM 给各种信息提供了一个自然语言的中间表示，来汇总处理各种信息（或者这两点其实是一点？）。这是一种类似人类思维方式的处理，但是也带来一个问题，就是 LLM 能力的上限决定了基于 LLM 的自动驾驶模型的上限，人类自然语言体系表达能力的上限又决定了 LLM 能力的上限（尤其体现于那些基于 Q-A 的模型上，非常依赖自然语言表示）。LLM 固然是通用的，也确实是现在看来最通用的模型，给很多多模态问题提供了可以接受的解决方案，理论上可以达到按照自然语言进行思考的人类驾驶员的水平；但是要捅破这层上限，是否还需要其他的方式？

Anyway，在解决多模态问题，尤其是自动驾驶这样一个 multi-task 的问题方面，LLM 确实是当下最可靠的解决方案之一。话不再多说了，下面是笔记正文。

## 分类

因为自动驾驶要做的任务本身就比较多，所以LLM应用的场景也不一样。

比较自然产生的是 **Perception** 和 **Planning & Control** 。紧接着有包括世界模型（比如[之前看过的这篇](https://meteorcollector.github.io/2024/08/end-to-end-recent/#driving-into-the-future-multiview-visual-forecasting-and-planning-with-world-model-for-autonomous-driving)）等应用方式的生成式 **Generation (Diffusion)**  LLM。然后是基于问答的 **Question Answering (QA)** 用 LLM，最后为了评估，还有 **Evaluation & Benchmark**。

<p><img src="{{site.url}}/images/DriveCite.png" width="100%" align="middle" /></p>

## Goole Scolar Citations (up to 2024/9/9)

### Perception: Tracking

48  Language prompt for autonomous driving

### Perception: Detection

39  Hilm-d: Towards high-resolution understanding in multimodal large language models for autonomous driving

### Perception: Prediction

17  Can you text what is happening? integrating pre-trained language encoders into trajectory prediction models for autonomous driving

17  Mtd-gpt: A multi-task decision-making gpt model for autonomous driving at unsignalized intersections

5   LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models

0   LeGo-Drive: Language-enhanced Goal-oriented Closed-Loop End-to-End Autonomous Driving

2   Large Language Models Powered Context-aware Motion Prediction

### Planning & Control: Prompt Engineering

13  Empowering Autonomous Driving with Large Language Models: A Safety Perspective

10  Personalized Autonomous Driving with Large Language Models: Field Experiments (name modified)

35  ChatGPT as your vehicle co-pilot: An initial attempt

40  Receive, reason, and react: Drive as you say, with large language models in autonomous vehicles

86  Languagempc: Large language models as decision makers for autonomous driving

32  Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving

34  Surrealdriver: Designing generative driver agent simulation framework in urban contexts based on large language model

67  Drive as you speak: Enabling human-like interaction with large language models in autonomous vehicles

77  Dilu: A knowledge-driven approach to autonomous driving with large language models

5    LLM-assisted light: Leveraging large language model capabilities for human-mimetic traffic signal control in complex urban environments

11  AccidentGPT: Accident analysis and prevention from V2X environmental perception with multi-modal large model

20  Llm-assist: Enhancing closed-loop planning with language-based reasoning

7    Driving Everywhere with Large Language Model Policy Adaptation

### Planning & Control: Fine-tuning Pre-trained Model

40  Drivemlm: Aligning multi-modal large language models with behavioral planning states for autonomous driving

48  Lmdrive: Closed-loop end-to-end driving with large language models

38  A language agent for autonomous driving

119 Gpt-driver: Learning to drive with gpt

69  Drivelm: Driving with graph visual question answering

114 Drivegpt4: Interpretable end-to-end autonomous driving via large language model

79  Driving with llms: Fusing object-level vector modality for explainable autonomous driving

17  Mtd-gpt: A multi-task decision-making gpt model for autonomous driving at unsignalized intersections

0    KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large Language Models

1    Asynchronous Large Language Model Enhanced Planner for Autonomous Driving

34  Drivevlm: The convergence of autonomous driving and large vision-language models

16  Rag-driver: Generalisable driving explanations with retrieval-augmented in-context learning in multi-modal large language model

9    VLP: Vision Language Planning for Autonomous Driving

11  Dme-driver: Integrating human decision logic and 3d scene perception in autonomous driving

### Generation (Diffusion)

17  Adriver-i: A general world model for autonomous driving

16  DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model

59  Drivedreamer: Towards real-world-driven world models for autonomous driving

30  Language-guided traffic simulation via scene-level diffusion

90  Gaia-1: A generative world model for autonomous driving

39  Magicdrive: Street view generation with diverse 3d geometry control

24  Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving

0   ChatScene: Knowledge-Enabled Safety-Critical Scenario Generation for Autonomous Vehicles

1   REvolve: Reward Evolution with Large Language Models for Autonomous Driving

11 Genad: Generative end-to-end autonomous driving

7   Drivedreamer-2: Llm-enhanced world models for diverse driving video generation

12  Editable scene simulation for autonomous driving via collaborative llm-agents

5    LLM-assisted light: Leveraging large language model capabilities for human-mimetic traffic signal control in complex urban environments

3    LangProp: A code optimization framework using Large Language Models applied to driving

### Question Answering (QA): Visual QA

40  Drivemlm: Aligning multi-modal large language models with behavioral planning states for autonomous driving

69  Drivelm: Driving with graph visual question answering

12  Reason2drive: Towards interpretable and chain-based reasoning for autonomous driving

12  Lingoqa: Video question answering for autonomous driving

26  Dolphins: Multimodal language model for driving

0    A Superalignment Framework in Autonomous Driving with Large Language Models

3    Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving

9     Transgpt: Multi-modal generative pre-trained transformer for transportation

### Traditional QA

8    Domain knowledge distillation from large language model: An empirical study in the autonomous driving domain

14  Human-centric autonomous systems with llms for user command reasoning

4    Engineering safety requirements for autonomous driving with large language models

3    Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving

### Evaluation & Benchmark

46  On the road with gpt-4v (ision): Early explorations of visual-language model on autonomous driving

1   GPT-4V Takes the Wheel: Promises and Challenges for Pedestrian Behavior Prediction

15 Lampilot: An open benchmark dataset for autonomous driving with language model programs

6   Evaluation of large language models for decision making in autonomous driving

0   Testing Large Language Models on Driving Theory Knowledge and Skills for Connected Autonomous Vehicles

1   Probing Multimodal LLMs as World Models for Driving

9   Embodied understanding of driving scenarios

8   LimSim++: A Closed-Loop Platform for Deploying Multimodal LLMs in Autonomous Driving

7   OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning

2   AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving

## Datasets

工欲善其事，必先利其器。所以在这里稍微调整一下原综述论文的顺序，先来看数据集。

<p><img src="{{site.url}}/images/datatable.png" width="100%" align="middle" /></p>

可以看到绝大多数还是人力标注的。

#### BDD-X (2018)

Berkeley Deep Drive-X (eXplanation) Dataset

链接：https://github.com/JinkyuKimUCB/BDD-X-dataset

这是一个解释驾驶员行为的数据集。对于视频信息，标注的格式是一个description（驾驶员在做什么）和一个explanation（驾驶员为什么这样做）

We focus on generating textual descriptions and explanations, such as the pair:
*“Vehicle slows down”* (description) and *“Because it is approaching an intersection and the light is red”* (explanation)

explanation的设置还是很有必要的，可以用于避免一些倒果为因的问题。（想起了 [CILRS](https://meteorcollector.github.io/2024/08/end-to-end-classic/#cilrs-exploring-the-limitations-of-behavior-cloning-for-autonomous-driving-iccv-2019)）

#### HAD (2019)

Honda Research Institute-Advice

链接：https://usa.honda-ri.com/had

同样地，输入是运行信息，但是除了视频信息之外还提供了 [CAN-BUS](https://www.emqx.com/zh/blog/can-bus-how-it-works-pros-and-cons) 控制信息。嘶，这个控制信息是所有型号的车辆都是统一的吗？还是说翻译成 Honda 的车的 Behaviour 再使用？有一些神奇。

标注的信息是“Advice”，（1）**目标导向的建议（自上而下信号）**——用于影响车辆在导航任务中的行为；（2）**刺激驱动的建议（自下而上信号）**——传达一些视觉刺激，这些刺激是用户期望车辆控制器主动关注的。

原文：

> Advices consist of (1) goal-orientedadvice (top-down signal) - to influencethe vehicle in a avigation task and (2) stimulus-driven advice (bottom-upsignal) - conveys some visual stimulithat the user expects their attention tobe actively looked by the vehicle
> controller

#### Talk2Car (2019)

链接：https://talk2car.github.io/

Talk2Car 是基于 nuScenes 的，按官网的说法是加上了更多模态的传感器输入（其实我记得现在 nuScenes 的模态已经不少了，在那时候只有图像和 bounding boxes 吗？）。最主要的是 “Talk”：标注信息中有对车辆下的指令，“we consider an autonomous driving setting, where a passenger can control the actions of an Autonomous Vehicle by giving commands in natural language.”

例子：

>  You can park up ahead behind the **silver car**, next to that lamp post with the orange sign on it

同时在图像输入中，silver car 被标注。

#### DriveLM (2023)

OpenDriveLab的工作。链接：https://github.com/OpenDriveLab/DriveLM

这个数据集是基于 Carla 和 nuScenes 的。

> **DriveLM-Data** is the *first* language-driving dataset facilitating the full stack of driving tasks with graph-structured logical dependencies.

准确来讲，这个“全栈”指的是 Perception Prediction Planning 三项工作，而且组织方式是图结构（想起来了 nuScenes 数据集令人痛苦的图结构）

关于这个图结构，原仓库里有一个视频：

>  The most exciting aspect of the dataset is that the questions and answers (`QA pairs`) are connected in a graph-style structure, with QA pairs as every node and potential logical progression as the edges. The reason for doing this in the AD domain is that AD tasks are well-defined per stage, from raw sensor input to final control action through perception, prediction and planning.

>  Its key difference to prior VQA tasks for AD is the availability of logical dependencies between QAs, which can be used to guide the answering process. Below is a demo video illustrating the idea.

https://github.com/OpenDriveLab/DriveLM/assets/54334254/988472a8-d7b9-4685-b4b8-7a0e77f68265

<p><img src="{{site.url}}/images/drivelm.png" width="100%" align="middle" /></p>

在 perception 的 Q-A 对中，对所有关心的物体的运动状态进行发问，获取运动状态；在 prediction 中依据 perception 问出来的 condition 预测它们的行为；接着再利用 perception 这里问出来的结论进行 plan。

#### DRAMA (2023)



#### Rank2Tell (2023)

#### NuPrompt (2023)

#### NuScenes-QA (2023)

#### Reason2Drive (2023)

#### LingoQA (2023)

#### NuInstruct (2024)

#### OpenDV-2K (2024)

## Perception

## Planning & Control

## Generation

## Question Answering (QA)

## Evaluation & Benchmark

## 其他参考资料

https://medium.com/@jasonyen1009/ml-llm4drive-a-survey-of-large-language-models-for-autonomous-driving-faf6a6d3a954

