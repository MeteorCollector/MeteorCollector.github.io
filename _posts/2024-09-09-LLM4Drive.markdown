---
layout: post
title:  "LLM4Drive 笔记：自动驾驶用大语言模型"
date:   2024-09-09 11:58:00 +0800
categories: posts
tag: autonomous driving
---

## 前情提要

这是一篇从 [LLM4Drive: A Survey of Large Language Models for Autonomous Driving](https://arxiv.org/abs/2311.01043) 伸展出去的笔记，主要看看自动驾驶用大语言模型。唉，大语言模型，[LMDrive](https://meteorcollector.github.io/2024/08/end-to-end-recent/#lmdrive-closed-loop-end-to-end-driving-with-large-language-models) 那篇文章对 LLM 的赞誉之词还历历在目。

> ... large language models (LLM) have shown impressive reasoning capabilities that approach "Artificial General Intelligence"...
>
> ...大语言模型在目前已经拥有了接近“通用人工智能”的惊艳表现...

看到如此正面的描述总是要从反面思考一下。在我看来，除了 LLM 模型结构本身适宜多模态数据处理（用不同的embedding当自然语言一样处理就可以了）之外，在比较抽象的认识层面上，LLM 给各种信息提供了一个自然语言的中间表示，来汇总处理各种信息（或者这两点其实是一点？）。这是一种类似人类思维方式的处理，但是也带来一个问题，就是 LLM 能力的上限决定了基于 LLM 的自动驾驶模型的上限，人类自然语言体系表达能力的上限又决定了 LLM 能力的上限（尤其体现于那些基于 Q-A 的模型上，非常依赖自然语言表示）。LLM 固然是通用的，也确实是现在看来最通用的模型，给很多多模态问题提供了可以接受的解决方案，理论上可以达到按照自然语言进行思考的人类驾驶员的水平；但是要捅破这层上限，是否还需要其他的方式？

Anyway，在解决多模态问题，尤其是自动驾驶这样一个 multi-task 的问题方面，LLM 确实是当下最可靠的解决方案之一。话不再多说了，下面是笔记正文。

## 分类

因为自动驾驶要做的任务本身就比较多，所以LLM应用的场景也不一样。

比较自然产生的是 **Perception** 和 **Planning & Control** 。紧接着有包括世界模型（比如[之前看过的这篇](https://meteorcollector.github.io/2024/08/end-to-end-recent/#driving-into-the-future-multiview-visual-forecasting-and-planning-with-world-model-for-autonomous-driving)）等应用方式的生成式 **Generation (Diffusion)**  LLM。然后是基于问答的 **Question Answering (QA)** 用 LLM，最后为了评估，还有 **Evaluation & Benchmark**。

<p><img src="{{site.url}}/images/DriveCite.png" width="100%" align="middle" /></p>

## Goole Scolar Citations (up to 2024/9/9)

### Perception: Tracking

48  Language prompt for autonomous driving

### Perception: Detection

39  Hilm-d: Towards high-resolution understanding in multimodal large language models for autonomous driving

### Perception: Prediction

17  Can you text what is happening? integrating pre-trained language encoders into trajectory prediction models for autonomous driving

17  Mtd-gpt: A multi-task decision-making gpt model for autonomous driving at unsignalized intersections

5   LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models

0   LeGo-Drive: Language-enhanced Goal-oriented Closed-Loop End-to-End Autonomous Driving

2   Large Language Models Powered Context-aware Motion Prediction

### Planning & Control: Prompt Engineering

13  Empowering Autonomous Driving with Large Language Models: A Safety Perspective

10  Personalized Autonomous Driving with Large Language Models: Field Experiments (name modified)

35  ChatGPT as your vehicle co-pilot: An initial attempt

40  Receive, reason, and react: Drive as you say, with large language models in autonomous vehicles

86  Languagempc: Large language models as decision makers for autonomous driving

32  Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving

34  Surrealdriver: Designing generative driver agent simulation framework in urban contexts based on large language model

67  Drive as you speak: Enabling human-like interaction with large language models in autonomous vehicles

77  Dilu: A knowledge-driven approach to autonomous driving with large language models

5    LLM-assisted light: Leveraging large language model capabilities for human-mimetic traffic signal control in complex urban environments

11  AccidentGPT: Accident analysis and prevention from V2X environmental perception with multi-modal large model

20  Llm-assist: Enhancing closed-loop planning with language-based reasoning

7    Driving Everywhere with Large Language Model Policy Adaptation

### Planning & Control: Fine-tuning Pre-trained Model

40  Drivemlm: Aligning multi-modal large language models with behavioral planning states for autonomous driving

48  Lmdrive: Closed-loop end-to-end driving with large language models

38  A language agent for autonomous driving

119 Gpt-driver: Learning to drive with gpt

69  Drivelm: Driving with graph visual question answering

114 Drivegpt4: Interpretable end-to-end autonomous driving via large language model

79  Driving with llms: Fusing object-level vector modality for explainable autonomous driving

17  Mtd-gpt: A multi-task decision-making gpt model for autonomous driving at unsignalized intersections

0    KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large Language Models

1    Asynchronous Large Language Model Enhanced Planner for Autonomous Driving

34  Drivevlm: The convergence of autonomous driving and large vision-language models

16  Rag-driver: Generalisable driving explanations with retrieval-augmented in-context learning in multi-modal large language model

9    VLP: Vision Language Planning for Autonomous Driving

11  Dme-driver: Integrating human decision logic and 3d scene perception in autonomous driving

### Generation (Diffusion)

17  Adriver-i: A general world model for autonomous driving

16  DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model

59  Drivedreamer: Towards real-world-driven world models for autonomous driving

30  Language-guided traffic simulation via scene-level diffusion

90  Gaia-1: A generative world model for autonomous driving

39  Magicdrive: Street view generation with diverse 3d geometry control

24  Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving

0   ChatScene: Knowledge-Enabled Safety-Critical Scenario Generation for Autonomous Vehicles

1   REvolve: Reward Evolution with Large Language Models for Autonomous Driving

11 Genad: Generative end-to-end autonomous driving

7   Drivedreamer-2: Llm-enhanced world models for diverse driving video generation

12  Editable scene simulation for autonomous driving via collaborative llm-agents

5    LLM-assisted light: Leveraging large language model capabilities for human-mimetic traffic signal control in complex urban environments

3    LangProp: A code optimization framework using Large Language Models applied to driving

### Question Answering (QA): Visual QA

40  Drivemlm: Aligning multi-modal large language models with behavioral planning states for autonomous driving

69  Drivelm: Driving with graph visual question answering

12  Reason2drive: Towards interpretable and chain-based reasoning for autonomous driving

12  Lingoqa: Video question answering for autonomous driving

26  Dolphins: Multimodal language model for driving

0    A Superalignment Framework in Autonomous Driving with Large Language Models

3    Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving

9     Transgpt: Multi-modal generative pre-trained transformer for transportation

### Traditional QA

8    Domain knowledge distillation from large language model: An empirical study in the autonomous driving domain

14  Human-centric autonomous systems with llms for user command reasoning

4    Engineering safety requirements for autonomous driving with large language models

3    Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving

### Evaluation & Benchmark

46  On the road with gpt-4v (ision): Early explorations of visual-language model on autonomous driving

1   GPT-4V Takes the Wheel: Promises and Challenges for Pedestrian Behavior Prediction

15 Lampilot: An open benchmark dataset for autonomous driving with language model programs

6   Evaluation of large language models for decision making in autonomous driving

0   Testing Large Language Models on Driving Theory Knowledge and Skills for Connected Autonomous Vehicles

1   Probing Multimodal LLMs as World Models for Driving

9   Embodied understanding of driving scenarios

8   LimSim++: A Closed-Loop Platform for Deploying Multimodal LLMs in Autonomous Driving

7   OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning

2   AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving

## Datasets

工欲善其事，必先利其器。所以在这里稍微调整一下原综述论文的顺序，先来看数据集。

<p><img src="{{site.url}}/images/datatable.png" width="80%" align="middle" /></p>

可以看到绝大多数还是人力标注的。

#### BDD-X (2018) (Planning + VQA)

Berkeley Deep Drive-X (eXplanation) Dataset

链接：[https://github.com/JinkyuKimUCB/BDD-X-dataset](https://github.com/JinkyuKimUCB/BDD-X-dataset)

这是一个解释驾驶员行为的数据集。对于视频信息，标注的格式是一个description（驾驶员在做什么）和一个explanation（驾驶员为什么这样做）

We focus on generating textual descriptions and explanations, such as the pair:
*“Vehicle slows down”* (description) and *“Because it is approaching an intersection and the light is red”* (explanation)

explanation的设置还是很有必要的，可以用于避免一些倒果为因的问题。（想起了 [CILRS](https://meteorcollector.github.io/2024/08/end-to-end-classic/#cilrs-exploring-the-limitations-of-behavior-cloning-for-autonomous-driving-iccv-2019)）

#### HAD (2019) (Planning + Perception)

Honda Research Institute-Advice

链接：[https://usa.honda-ri.com/had](https://usa.honda-ri.com/had)

同样地，输入是运行信息，但是除了视频信息之外还提供了 [CAN-BUS](https://www.emqx.com/zh/blog/can-bus-how-it-works-pros-and-cons) 控制信息。嘶，这个控制信息是所有型号的车辆都是统一的吗？还是说翻译成 Honda 的车的 Behaviour 再使用？有一些神奇。

标注的信息是“Advice”，（1）**目标导向的建议（自上而下信号）**——用于影响车辆在导航任务中的行为；（2）**刺激驱动的建议（自下而上信号）**——传达一些视觉刺激，这些刺激是用户期望车辆控制器主动关注的。

原文：

> Advices consist of (1) goal-orientedadvice (top-down signal) - to influencethe vehicle in a avigation task and (2) stimulus-driven advice (bottom-upsignal) - conveys some visual stimulithat the user expects their attention tobe actively looked by the vehicle
> controller

#### Talk2Car (2019) (Planning + Perception)

链接：[https://talk2car.github.io/](https://talk2car.github.io/)

Talk2Car 是基于 nuScenes 的，按官网的说法是加上了更多模态的传感器输入（其实我记得现在 nuScenes 的模态已经不少了，在那时候只有图像和 bounding boxes 吗？）。最主要的是 “Talk”：标注信息中有对车辆下的指令，“we consider an autonomous driving setting, where a passenger can control the actions of an Autonomous Vehicle by giving commands in natural language.”

例子：

>  You can park up ahead behind the **silver car**, next to that lamp post with the orange sign on it

同时在图像输入中，silver car 被标注。

#### DriveLM (2023) (Perception + Prediction + Planning + VQA)

OpenDriveLab的工作。链接：[https://github.com/OpenDriveLab/DriveLM](https://github.com/OpenDriveLab/DriveLM)

这个数据集是基于 Carla 和 nuScenes 的。

> **DriveLM-Data** is the *first* language-driving dataset facilitating the full stack of driving tasks with graph-structured logical dependencies.

准确来讲，这个“全栈”指的是 Perception Prediction Planning 三项工作，而且组织方式是图结构（想起来了 nuScenes 数据集令人痛苦的图结构）

关于这个图结构，原仓库里有一个视频：[在github观看](https://github.com/OpenDriveLab/DriveLM/blob/main/docs/gvqa.md)    [直接下载](https://github.com/OpenDriveLab/DriveLM/assets/54334254/988472a8-d7b9-4685-b4b8-7a0e77f68265)

>  The most exciting aspect of the dataset is that the questions and answers (`QA pairs`) are connected in a graph-style structure, with QA pairs as every node and potential logical progression as the edges. The reason for doing this in the AD domain is that AD tasks are well-defined per stage, from raw sensor input to final control action through perception, prediction and planning.

>  Its key difference to prior VQA tasks for AD is the availability of logical dependencies between QAs, which can be used to guide the answering process. Below is a demo video illustrating the idea.

<p><img src="{{site.url}}/images/drivelm.png" width="100%" align="middle" /></p>

在 perception 的 Q-A 对中，对所有关心的物体的运动状态进行发问，获取运动状态；在 prediction 中依据 perception 问出来的 condition 预测它们的行为；接着再利用 perception 这里问出来的结论进行 plan。

#### DRAMA (2023) (VQA)

和 HAD 一样又是 honda 的工作。链接：[https://usa.honda-ri.com/drama](https://usa.honda-ri.com/drama)

这个数据集是由很多两秒钟的视频片段组成的，在东京收集。

输入是视频、CAN控制信息、IMU信息；

标注是 Video-levelQ/A, Object-level Q/A, Risk object boundingbox, Free-form caption, and separate labelsfor ego-car intention, scene classifier and suggestions to the driver.

<p><img src="{{site.url}}/images/DRAMA.png" width="80%" align="middle" /></p>

#### Rank2Tell (2023) (Perception + VQA)

又是 honda 的工作，链接：[https://usa.honda-ri.com/rank2tell](https://usa.honda-ri.com/rank2tell)

20秒的116个片段，10fps（有点小啊），

输入：图像，LiDAR，GPS；

标注：Video-level Q/A, Object-level Q/A, LiDAR and 3D bounding boxes (with tracking), Field of view from 3 cameras (stitched), important object bounding boxes (multiple important objects per frame with multiple levels of importance-High, Medium, Low), free-form captions (multiple captions per object for multiple objects), ego-car intention.

<p><img src="{{site.url}}/images/Rank2Tell.jpg" width="80%" align="middle" /></p>

#### NuPrompt (2023) (Perception)

链接：[https://github.com/wudongming97/Prompt4Driving](https://github.com/wudongming97/Prompt4Driving)

从名字就能看出来基于 nuScenes，当然下面那个也是（事实上从论文的引用情况来看，NuScenes-QA 早于 NuPrompt）。值得一提的是这个数据集的生成用上了 ChatGPT-3.5，有一个根据描述不断取交集的过程（根据不同的prompt组合出query），在论文里有所体现。相较于其他数据集，这个数据集有利于模型从跨场景的信息中学习（论文里是这么说的？），提高复杂情形下的理解能力。

> We assign a language prompt to a collection of objects sharing the same characteristics for grounding them. Essentially, this benchmark provides lots of 3D instance-text pairings with three primary attributes: ❶ Real-driving descriptions. Different from existing benchmarks that only represent 2D objects from modular images, the prompts of our dataset describe a variety of driving-related objects from 3D, looking around, and long-temporal space ... ❷ Instance-level prompt annotations. Every prompt indicates a fine-grained and discriminative object-centric description, as well as enabling it to cover an arbitrary number of driving objects. ❸ Large-scale language prompts. NuPrompt is comparable to the largest current dataset in terms of the number of prompts, i.e., including 35,367 language prompts.

#### NuScenes-QA (2023) (VQA)

竟然是复旦，链接：[https://github.com/qiantianwen/NuScenes-QA](https://github.com/qiantianwen/NuScenes-QA)

这个就很直观了，从标题就可以看出是基于 nuScenes 数据集的 VQA。

<p><img src="{{site.url}}/images/NuScenes-QA.png" width="80%" align="middle" /></p>

#### Reason2Drive (2023) (Perception + Prediction + VQA)

竟然还是复旦。不得不说复旦做的自动驾驶数据集还是挺多的。

链接：[https://github.com/fudan-zvg/Reason2Drive](https://github.com/fudan-zvg/Reason2Drive)

超级缝合加大版，用了 nuScenes，Waymo，ONCEA 这些数据集进行增广标注。之后 “parse their comprehensive object metadatas into JSON-structured entries. Each object entry contains various details pertaining to its driving actions.” 有了这些 metaData，就可以在多种任务的情况下，基于大语言模型进行增广，获得 QA-pair。

<p><img src="{{site.url}}/images/Reason2Drive.png" width="80%" align="middle" /></p>

感觉基于大语言模型的 QA-pair 生成基本都是这种套路，对于这种数据的增广，LLM 还是比较高效的。想到自己之前做 iris 的数据集的时候也是这么干的（

#### LingoQA (2023) (VQA)

链接：[https://github.com/wayveai/LingoQA](https://github.com/wayveai/LingoQA)

一个大规模的VQA工作，标注上把GPT、人工以及各种方式全用上了。手笔最大的一集。

比较有意思的是它还包括了 Lingo-Judge，可以给LLM在自动驾驶任务中问题生成的结果打分（correctness，0-1取值）。

#### NuInstruct (2024) (Perception + Prediction + VQA)

链接：[https://github.com/xmed-lab/NuInstruct](https://github.com/xmed-lab/NuInstruct)

这篇review过了，在这里：[端到端自动驾驶笔记（最新进展篇） (meteorcollector.github.io)](https://meteorcollector.github.io/2024/08/end-to-end-recent/#holistic-autonomous-driving-understanding-by-birds-eye-view-injected-multi-modal-large-models)

这篇论文强调的是 Holistic，也就是整体、综合的。NuInstruct 比较关注整体的信息，分了17个子任务，涉及时间和空间上的组合、多视角等等信息。并且分了不同的类别：

> In our research, we propose an SQL-based approach for the automated generation of four types of instruction-follow data, namely: Perception, Prediction, Risk, and Planning with Reasoning.

数据格式示例：

```
Train
{
  'task': ...,  # the task type, e.g., risk-overtaking
  'qa_id':...,   # QA pairs ID
  'img_path':..., # image path list for a video clip
  'Question':...., # 
  'Answer':...,
  'sample_list':.... # sample token list of corresponding images in NuScense
}
```

#### OpenDV-2K (2024) (Perception + Prediction + VQA)

链接：[https://github.com/OpenDriveLab/DriveAGI](https://github.com/OpenDriveLab/DriveAGI)

究极巨大数据集。including 1747 hours from YouTube and 312 hours from public datasets, with automatically generated language annotations to support generalized video prediction model training. 事实上因为太大了，在 readme 上提出了建议：`It's recommended to set up your experiments on a small subset, say 1/20 of the whole dataset`

GenAD 本身是做世界模型 (World Model) 的工作，所以大规模的带标注的视频确实是这样的生成工作所需要的。

## Perception

以下论文我个人的阅读顺序是引用数从高到低，由细到粗。

### Language Prompt for Autonomous Driving

todo

### Hilm-d: Towards High-Resolution Understanding in Multimodal Language Models for Autonomous Driving

todo

### Mtd-gpt: A multi-task decision-making gpt model for autonomous driving at unsignalized intersections

todo

## Planning & Control

### Receive, reason, and react: Drive as you say, with large language models in autonomous vehicles

todo

### LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving (13 Oct 2023)

链接：[https://arxiv.org/abs/2310.03026](https://arxiv.org/abs/2310.03026)

<p><img src="{{site.url}}/images/LanguageMPC.png" width="80%" align="middle" /></p>

> From left to right, the LLM proceeds sequentially: 1) identifies the vehicles requiring attention, 2) evaluates the situation, and 3) offers action guidance. Then the system transforms these three **high-level textual decisions** into **mathematical representations**, namely the **observation matrix**, **weight matrix**, and **action bias**. These elements serve as directives for the bottom-level controller, the MPC, instructing it on specific driving actions to be taken. These elements serve as directives for the bottom-level controller, the MPC, instructing it on specific driving actions to be taken.

和 GPT-Driver 类似，也是分步走的工作。这里比较有意思的是，对于每一步（perception、prediction、planning）工作，模型要把它们转化为数学表示（observation matrix, weight matrix, action bias）。[MPC](https://levelup.gitconnected.com/model-predictive-control-for-autonomous-vehicle-an-in-depth-guide-de984308ba10) 是 Model Predictive Control 即模型预测控制。

> The MPC solves a finite-time open-loop optimization problem online at each moment, based on the current measurement information obtained, and applies the first element of the resulting control sequence with the lowest cost to the controlled vehicle.

MPC基于当前获取的测量信息，在每个时刻在线求解有限时间的开环优化问题，并将生成的控制序列中代价最低的第一个控制量应用于被控车辆。

### Drive as you speak: Enabling human-like interaction with large language models in autonomous vehicles

todo

### Dilu: A knowledge-driven approach to autonomous driving with large language models

todo

### LMDrive: Closed-Loop End-to-End Driving with Large Language Models

这篇看过了，详见 [端到端自动驾驶笔记（最新进展篇） (meteorcollector.github.io)](https://meteorcollector.github.io/2024/08/end-to-end-recent/#lmdrive-closed-loop-end-to-end-driving-with-large-language-models)

### A language agent for autonomous driving

todo

### GPT-Driver: Learning to Drive with GPT (5 Dec 2023)

链接：[https://arxiv.org/abs/2310.01415](https://arxiv.org/abs/2310.01415)

赵行的一篇，通篇没什么图片，看得我文字恐怖谷效应犯了。还有一大堆用公式形式化说明的内容，maths speak louder than words 吧大概。这篇着重关心的是 Planning 方面。

<p><img src="{{site.url}}/images/GPT-Driver.png" width="90%" align="middle" /></p>

The crucial insight of this paper is to transform motion planning into a language modeling problem. Given a driving trajectory $\mathcal{T}$, we can represent it as a sequence of words that describe this trajectory:

`$$\mathcal{T} = K(\{(x_1, y_1), \cdots, (x_{t}, y_{t})\}) = \{w_1, \cdots, w_n\},$$`

where `$w_i$` is the $i$-th word in this sequence. Please note that each coordinate value $x$ or $y$ in Equation  can be freely transformed into a set of words $\{w\}$ using a language tokenizer $K$.

这块算是从理论上阐释了用自然语言模型做路径规划的可行性吧。

#### 模型结构

GPT-Driver 得到路径的步骤是这样的（“novel prompting-reasoning-finetuning strategy”）：

- 用 GPT 的 tokenizer $K$ 把观测输入 $\mathcal{O}$ 和 ego-state $\mathcal{S}$ 转换成为 language prompt （不是，prompt 不还是自然语言吗？这块 tokenize 成啥了啊，我看到 tokenize 还以为和一些多模态模型一样将其他模态的信息编码。而且论文3.3部分的 prompting 子部分也确实说明了他们确实是直接把感知输入转化成自然语言描述进行处理的，非常直接。原文：we resort to the parameterized
  representations of observations and ego-states and convert them into language descriptions，感觉还是很 prompt engineering）。

<p><img src="{{site.url}}/images/GPT-DriverPrompt.png" width="70%" align="middle" /></p>

- 将 prompt 喂给 GPT3.5，记为 `$F_{GPT}$`，让语言模型根据 prompt 中的自然语言描述出 trajectory。这一步是 Reasoning：一步一步思考，给出预测的路径。现在用分模块端到端模型的视角看，第一步包含了 perception 比较后期和prediction的部分，后两步都属于 planning。实际上就是让大语言模型一步步干活——通过少量对话，可能大语言模型并不能立刻给出准确的答案，但是分成一个个子任务循序渐进，就可以让大语言模型以我们指定的逻辑链条进行推理，给出一个比较 reasonable 的答案。

> In particular, we summarize the chain-of-thought reasoning process in autonomous driving into $3$ steps: First, from the perception results, the motion planner needs to **identify those critical objects** that may affect its driving dynamics. Second, by analyzing the future motions of these critical objects from the prediction results, the **planner** should infer when, where, and how this critical object may influence the ego vehicle. Third, on top of the insights gained from the previous analyses, the planner needs to draw **a high-level driving decision **and then convert it into a planned trajectory.

<p><img src="{{site.url}}/images/GPT-DriverReasoning.png" width="70%" align="middle" /></p>

- We employ a simple finetuning strategy using the OpenAI fine-tuning API. 微调这个自然语言描述出的结果，使它与人类司机的驾驶轨迹接近。轨迹是直接从人类驾驶数据获得的，对于 reason-chain，用了一个 rule-based 的方法来获得 Ground Truth：直接计算车辆应该有的速度，关键物体的识别就是计算出与预测路径重合的物体。

总地来看，引用论文里的公式，就是：

`$$\{ \mathcal{T}, \mathcal{R} \} = F_{GPT}(K(\mathcal{O}, \mathcal{S}))$$`

#### 验证方式

由于是 planning 工作，使用的是开环 nuScenes，L2 + Collision，无需多言。从表上看，GPT-Driver 的表现比 Uni-AD 还好，有些惊艳。不过由于 OpenAI 的 API 是纯黑盒，所以也并不知道这个模型的推理时间是多少，有待以后再验证吧。

### Drivelm: Driving with graph visual question answering

todo

### DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model (14 Mar 2024)

链接：[https://arxiv.org/abs/2310.01412](https://arxiv.org/abs/2310.01412)

<p><img src="{{site.url}}/images/DriveGPT4.png" width="90%" align="middle" /></p>

#### 数据准备

这篇文章的数据集是以 BDD-X 为基础的，作者觉得 BDD-X 太机械了而且包含的信息少，因此使用 GPT-4 增广了 Q-A 对（we create our own dataset based on BDD-X assisted by ChatGPT）。这一步处理在原文中是这样写的：

To address the aforementioned issue, ChatGPT is leveraged as a teacher to generate more conversations about the ego vehicle. The prompt generally follows the prompt design used in LLaVA. （提示符通常遵循LLaVA中使用的提示符设计）To enable ChatGPT to "see" the video, YOLOv8 (Reis et al., 2023) is implemented to detect commonly seen objects in each frame of the video (e.g., vehicles, pedestrians) （为了使ChatGPT能够“看到”视频，使用了YOLOv8 (Reis et al.， 2023)来检测视频每帧中常见的物体(例如车辆、行人)）。Obtained bounding box coordinates are normalized following LLaVA and sent to ChatGPT as privileged information. （得到的边界框坐标**按照LLaVA进行归一化**（见下），并作为特权信息发送给ChatGPT）In addition to object detection results, the video clip's ground truth control signal sequences and captions are also accessible to ChatGPT. 除了目标检测结果外，ChatGPT还可以访问视频片段的ground truth和标注信息。Based on this privileged information, ChatGPT is prompted to generate multiple rounds and types of conversations about the ego vehicle, traffic lights, turning directions, lane changes, surrounding objects, spatial relations between objects, etc. 基于这些特权信息，ChatGPT可以生成关于ego vehicle、交通信号灯、转弯方向、车道变化、周围物体、物体之间的空间关系等多轮和类型的对话。

**所谓按照LLaVA进行归一化**应该按照LLaVA论文中生成数据的方式整理格式。在 LLaVA的训练过程中，先通过 vision model 得到 context，原论文的示例：Context type 1: Captions A group of people standing outside of a black vehicle with various luggage. Luggage surrounds a vehicle in an underground parking area People try to fit all of their luggage in an SUV. The sport utility vehicle is parked in the public garage, being packed for a trip Some people with luggage near a van that is transporting it. Context type 2: Boxes person: [0.681, 0.242, 0.774, 0.694], backpack: [0.384, 0.696, 0.485, 0.914], suitcase: ... 然后再输入 GPT-4 来增广 Q-A 对，让 GPT4 扮演教师的角色。

总而言之，训练数据格式是这样的：

<p><img src="{{site.url}}/images/DriveGPT4Data.png" width="80%" align="middle" /></p>

#### 模型结构

经典的把一切输入tokenize，concatenate，然后塞入大语言模型。

**Video**

DriveGPT4 要输入视频，使用的视频 tokenizer 是 Valley。

论文原文：

Let the input video frames be denoted as `$V = [I_1, I_2, \ldots, I_N]$`. For each video frame `$I_i$`, the pretrained **CLIP visual encoder** (Radford et al., 2021) is used to extract its feature `$F_i \in R^{257\times d}$`. The first channel of `$F_i$` represents the **global feature** of `$I_i$`, while the other 256 channels correspond to **patch features** of `$I_i$`. For succinct representation, the **global feature** of `$I_i$` is denoted as `$F^G_i$` , while the local patch features of `$I_i$` are represented as `$F^P_i$` . The temporal visual feature of the entire video can then be expressed as:

`$$T = F^G_0 \oplus F^G_1 \oplus \ldots \oplus F^G_N$$`

where $\oplus$ denotes The spatial visual feature of the whole video is given by:

`$$S = \mathrm{Pooling}\left(F^P_0, F^P_1, \ldots, F^P_N\right)$$`

where $\mathrm{Pooling}(\cdot)$ represents a pooling layer that convert $N$ features into a single feature tensor for memory efficiency. Ultimately, both the temporal feature $T$ and spatial feature $S$ are projected into the text domain using a **projector**.

**Text and control signals**

为了进行自然语言问询和控制信号的预测，控制信号也要被输入，文中说是“与文本类似”的处理方法。

#### 训练过程

这么大的模型没办法整体完全训练。于是很常见地分为了两步：

**Pretraining**

这一步 CLIP 和 LLM 被冻结，只有 **projector** 被训练。

使用的数据是 593K image-text pairs from the [CC3M dataset](https://ai.google.com/research/ConceptualCaptions/download) and 703K video-text pairs from the [WebVid-2M dataset](https://huggingface.co/datasets/luoruipu1/Valley-webvid2M-Pretrain-703K) (Bain et al., 2021). 我看了一下，前者是各种图片的描述，后者是一些 vision q-a 对，是针对视频的。在我看来预训练阶段是对 perception 部分的先行训练。

**Mix-finetune**

在这一阶段，projector、LLM 都被训练，目的是 “enable DriveGPT4 to understand and process domain knowledge”。这一步就用到了**数据准备**部分生成的数据了，一共 56K；同时还使用了 223K general instruction-following data generated by LLaVA and Valley。The former ensures that DriveGPT4 can be applied for interpretable end-to-end autonomous driving, while the latter enhances the data diversity and visual understanding ability of DriveGPT4.

#### 验证方式

**Interpretable AD**

Ground Truth 用的主要是 BDD-X，因为“基本没有别的可用”。在这篇文章之前，SOTA方法是 [ADAPT](https://arxiv.org/abs/2302.00673) ，因此文章里主要将 DriveGPT4 和 ADAPT 做对比。采用的指标是 NLP 那边比较常用的，包括 CIDEr，BLEU4，ROUGE-L。这三者的描述和实现可以参考 [https://github.com/Aldenhovel/bleu-rouge-meteor-cider-spice-eval4imagecaption](https://github.com/Aldenhovel/bleu-rouge-meteor-cider-spice-eval4imagecaption) ，都是衡量和 Ground Truth 相似度用的，用来打分。

**End-to-end Control**

Ground Truth 仍使用 BDD-X 。衡量了控制信号（速度和转向角）的预测能力，使用的是均方误差（$MSE$）和 threshold accuracy （`$A_\gamma$`）（the proportion of test samples with prediction errors lower than $\gamma$）

**定性分析**

剩下的就是一些定性的展示，比如和 GPT-4V 比一比理解能力之类，不在这里费笔墨了。

### Driving with llms: Fusing object-level vector modality for explainable autonomous driving

todo

### VLP: Vision Language Planning for Autonomous Driving

这篇看过了，详见 [端到端自动驾驶笔记（最新进展篇） (meteorcollector.github.io)](https://meteorcollector.github.io/2024/08/end-to-end-recent/#vlp-vision-language-planning-for-autonomous-driving)

## Generation

### Drivedreamer: Towards real-world-driven world models for autonomous driving

todo

###  Gaia-1: A generative world model for autonomous driving

todo

### Magicdrive: Street view generation with diverse 3d geometry control

todo

### Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving

这篇看过了，详见 [端到端自动驾驶笔记（最新进展篇） (meteorcollector.github.io)](https://meteorcollector.github.io/2024/08/end-to-end-recent/#driving-into-the-future-multiview-visual-forecasting-and-planning-with-world-model-for-autonomous-driving)

## Question Answering (QA)

### Drivemlm: Aligning multi-modal large language models with behavioral planning states for autonomous driving'

todo

### Drivelm: Driving with graph visual question answering

见上

### Human-centric autonomous systems with llms for user command reasoning

todo

## Evaluation & Benchmark

### On the road with gpt-4v (ision): Early explorations of visual-language model on autonomous driving

todo

## 其他参考资料

一个丐版的原论文笔记：[https://medium.com/@jasonyen1009/ml-llm4drive-a-survey-of-large-language-models-for-autonomous-driving-faf6a6d3a954](https://medium.com/@jasonyen1009/ml-llm4drive-a-survey-of-large-language-models-for-autonomous-driving-faf6a6d3a954)