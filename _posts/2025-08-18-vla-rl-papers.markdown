---
layout: post
title:  "看看论文"
date:   2025-08-18 14:30:00 +0800
categories: posts
tag: autonomous driving
---

# 看看最近的论文（2025年八月下半月）

> 一般来讲，越靠上的越新

## VLA

### Video Generators are Robot Policies

> 直接从视频转化成动作，比较简单的思路

> “**只要让大视频扩散模型学会“想象”机器人完成任务的全过程，再用一个小解码器把想象转成动作，就能用极少的演示数据获得远超传统模仿学习的泛化能力。**”

#### 🎯 研究动机  
1. **行为克隆（BC）** 需要海量真人演示，且跨物体/场景/任务迁移差。  
2. **互联网级视频扩散模型**（Sora 类）已学会物理与语义先验，却只用来“看”，没拿来“干”。  
3. 能否把“生成未来帧”直接当成策略？——即 **Video Generator ≈ Robot Policy**。

#### 🧩 方法框架：Video Policy  

| 组成 | 作用 | 技术细节 |
|---|---|---|
| **Video U-Net (μθ)** | 想象未来 8–32 帧 | 以 SVD 为骨干，输入首帧+任务文本 → 生成“机器人完成任务”的视频 |
| **Action U-Net (αθ)** | 把想象转成动作 | 轻量 1D-CNN，从 μθ 中间特征解码 6-DoF 轨迹 + 夹爪 |
| **两阶段训练** | 先训视频，后训动作 | 冻结 μθ，仅用 50–300 条演示即可训动作头；梯度不回传，避免稀释视频先验 |

#### 🏗️ 核心洞见  
- **先训视频再训动作** > 端到端联合训练（RoboCasa +9 %）。  
- **视频预测 horizon 越长** → 对分布漂移越鲁棒（图 3）。  
- **无动作视频也能辅助泛化**：仅用 12 任务动作数据，但视频见过全部 24 任务 → 在未见任务上仍 >0.5 成功率（图 4）。  

#### 📊 实验结果  

| Benchmark | 演示数 | Video Policy | 最强基线 | 提升 |
|---|---|---|---|---|
| **RoboCasa (34 任务)** | 50 | **66 %** 平均成功率 | DP-ResNet 41 % | +25 % |
| **Libero-10** | 50 | **94 %** 平均成功率 | UVA 90 % | +4 % |
| **真实世界 5 任务** | 200/任务 | 80–100 % （位置/物体/背景漂移） | — | 显著 |

#### 🔍 泛化维度验证  
| 维度 | 举例 | 成功率 | 说明 |
|---|---|---|---|
| **位置漂移** | 抽屉/杯子随机摆放 | 80–100 % | 视频先验鲁棒 |
| **未见物体** | 异形杯、彩色 M&M | 70–90 % | 形状/颜色泛化 |
| **背景变化** | 黑/红/蓝桌面 | 80 %± | 透明杯在低对比度场景略降 |

#### ⚖️ 局限与展望  
- **计算大**：25 帧 × 256² 需 9 s（A100），未来靠蒸馏/加速可实时。  
- **单臂单任务**：暂未做多臂、长时程、语言指令。  
- **模型单一**：仅基于 SVD；后续可试更大视频-语言-动作预训练模型。


> **把“想象机器人怎么做”的像素级扩散模型，当成策略本体；再配一个极轻量的动作解码器，就能用 50 条演示打败 3000 条演示的传统 BC，并轻松泛化到全新物体、场景和任务。**

### IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model

[arxiv](https://arxiv.org/abs/2508.06571) [github (5天前开源，16stars)](https://github.com/IRL-VLA/IRL-VLA)

> 训练世界模型进行 RL 训练

这篇论文提出了一种全新的端到端自动驾驶框架——**IRL-VLA**（Inverse Reinforcement Learning for Vision-Language-Action），它通过**逆强化学习构建奖励世界模型（Reward World Model, RWM）**，实现了**不依赖高保真仿真器的闭环强化学习训练**，显著提升了自动驾驶系统在复杂场景下的表现。

#### 🎯 **研究背景与挑战**

当前主流的端到端自动驾驶系统（如UniAD、VAD、DiffusionDrive等）大多基于**模仿学习（Imitation Learning）**，存在两个关键缺陷：

1. **开环训练**：模型只是“模仿”人类驾驶行为，无法主动探索更优策略，容易复制数据集中的次优行为。
2. **闭环训练困难**：传统闭环训练依赖高保真仿真器，存在**Sim2Real域差**和**计算开销巨大**的问题。

#### 🧩 **IRL-VLA 的核心思想**

IRL-VLA 提出了一种**三阶段训练范式**，巧妙避开了上述问题：

| 阶段 | 名称 | 关键内容 |
|------|------|----------|
| **阶段1** | 模仿预训练 | 构建一个强大的Vision-Language-Action（VLA）模型，通过人类驾驶数据进行模仿学习，建立基础策略。 |
| **阶段2** | 逆环境学习 | 通过**逆强化学习（IRL）**训练一个**轻量级奖励世界模型（RWM）**，用于预测任意轨迹的奖励（如安全性、舒适性、效率）。 |
| **阶段3** | 闭环强化学习 | 利用RWM作为奖励来源，采用**PPO（Proximal Policy Optimization）**算法对VLA策略进行微调，实现**不依赖仿真器的闭环训练**。 |

#### 🧠 **技术细节亮点**

1. **VLA模型架构**
- **语义推理模块**：基于SennaVLM，处理多视角图像和语言指令，理解场景语义。
- **3D推理模块**：将图像特征投影到BEV（鸟瞰图）空间，提取地图和动态目标信息。
- **统一扩散规划器**：采用扩散模型生成多模态轨迹，具备更强的泛化能力。

2. **奖励世界模型（RWM）**
- **输入**：多视角图像 + 预测轨迹。
- **输出**：预测8个驾驶指标（如无碰撞、车道保持、交通灯合规等），加权得到最终奖励。
- **优势**：
  - **无需仿真器**：直接基于真实数据预测奖励，避免Sim2Real域差。
  - **轻量级**：相比传统仿真器，计算效率提升显著。

3. **强化学习训练**
- **算法**：PPO（稳定、样本高效）。
- **奖励来源**：RWM实时预测。
- **策略优化**：结合模仿学习和强化学习损失，避免灾难性遗忘。


#### 📊 **实验结果**

- **NAVSIM v2挑战赛**：在CVPR 2025自动驾驶大挑战中获得**第二名**，EDPMS得分45.0。
- **Navhard基准测试**：EPDMS得分74.9，超越DiffusionDrive（63.2）、WOTE（66.7）等方法。
- **消融实验**：
  - 加入语义推理模块，性能提升1.4 EPDMS。
  - 加入扩散规划器，性能提升3.0 EPDMS。
  - 模仿损失权重为0.5时，强化学习与模仿学习达到最佳平衡。

#### 🚀 **贡献与意义**

- **首次**实现了**不依赖仿真器的端到端VLA闭环强化学习**。
- 提出了**通用可扩展的奖励世界模型（RWM）**，为自动驾驶强化学习提供了新范式。
- 在多个基准测试中达到**SOTA性能**，验证了方法的有效性和泛化能力。



## RL

### EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving

> 这一篇 navsim 搞的很高，对抗网络还挺有意思的

EvaDrive 把“轨迹生成 + 多目标评价”做成一个**可迭代的对抗博弈**：生成器（Actor）不断提出候选轨迹，评价器（Critic）用**多维奖励向量**打分，二者在多轮 Pareto 优化里互相拉扯，最终产出的轨迹既安全舒适又高效，还不用人工标注偏好。

#### 🎯 研究痛点  
1. **模仿学习 / 单目标 RL** → 只能学平均行为，缺乏多样性和迭代试错。  
2. **现有生成-评价框架** → 生成和评价是“一次性”流水线，缺少闭环。  
3. **GRPO/DPO** → 把多维指标硬压成一个标量奖励，带来 scalarization bias。

#### 🧩 方案总览  
| 模块 | 职责 | 关键技术 |
|---|---|---|
| **Actor（生成器）** | 输出多样化轨迹 | ① 自回归意图建模（保留时间因果）<br>② 单步扩散精炼（空间灵活） |
| **Critic（评价器）** | 给轨迹打分 | 输出 **K 维奖励向量**（安全、舒适、效率…） |
| **多轮优化机制** | 让 Actor 与 Critic 来回迭代 | 每轮从 Pareto 前沿采样轨迹作为下一轮 anchor |
| **Adversarial Policy Optimization (APO)** | 让 Actor 与 Critic 对抗 | 类似 GAN：Actor 想拿高分，Critic 把专家轨迹打更高 |

#### 🏗️ 训练流程（算法 1 简述）  
```
for 轮次 t = 0…K-1:
    1. Actor 生成 64 条候选轨迹 At
    2. Critic 输出 K 维奖励 r(·)
    3. 用 Fast Non-Dominated Sort 建 Pareto 前沿 Pt
    4. 从 Pt 均匀采样 M 条作为下一轮 anchor
    5. 下一轮 Actor 以 anchor 为条件继续产出 At+1
```
最后一轮取 Pareto 最优轨迹作为最终输出。

#### 📊 实验战绩  
| 场景 | 关键指标 | EvaDrive | 对比基线 | 提升 |
|---|---|---|---|---|
| NAVSIM v1 | PDMS | **94.9** | DiffusionDrive 88.1 | +6.8 |
| NAVSIM v2 | EPDMS | **86.3** | Hydra-MDP++ 85.6 | +0.7 |
| Bench2Drive | Driving Score | **64.96** | DriveTransformer 65.02 | 基本打平，但风格可控 |

- **风格开关**：只改权重 w，即可在同模型上切换  
  - 保守型：安全权重高，PDMS=93.5  
  - 激进型：效率权重高，PDMS=94.9  
- **消融**（表 3）：去掉对抗/多轮/Pareto 任一环节都会掉点。

#### 🔍 亮点小结  
1. **第一次**在自动驾驶里把“多目标 + 多轮 + 对抗”完整跑通。  
2. **无需人工偏好对**，奖励直接来自仿真规则，规避 GRPO/DPO 的标注噪声。  
3. **单步扩散 + Pareto 采样**，兼顾实时性与多样性。  

> EvaDrive 让车像人一样“多想几步”，在 NAVSIM 上拿下新的 SOTA，同时可按权重旋钮切换驾驶风格。



### ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction

[arxiv](https://arxiv.org/abs/2508.08170) 暂未开源

> 算是一个有趣的思路吧。它先把场景做三维重建，这样agent无论在场景里如何移动，都可以先渲染个大概。对于动态物体例如其他交通参与者，就用 diffusion 渲染出来。

这篇论文提出了 **ReconDreamer-RL**，一个**利用扩散模型重建驾驶场景、增强强化学习训练效果**的端到端自动驾驶训练框架。

### 🎯 **核心问题**
现有自动驾驶强化学习训练面临两大瓶颈：

1. **仿真环境不真实**（Sim2Real Gap）  
   - 游戏引擎缺乏真实传感器数据；
   - 重建方法（如3DGS）在新轨迹/视角下渲染质量差。

2. **训练数据分布偏差**  
   - 模仿学习（IL）数据集中缺乏“cut-in”“急刹”等corner cases；
   - 强化学习冷启动困难，探索空间受限。

#### 🧩 **ReconDreamer-RL 的三件套**

| 模块 | 作用 | 技术亮点 |
|---|---|---|
| **ReconSimulator** | 构建高质量、可交互的仿真环境 | 3DGS + 视频扩散模型（DriveRestorer）提升新视角渲染质量；加入运动学模型保证轨迹物理合理性 |
| **DAA（动态对抗代理）** | 自动生成 corner cases | 控制周围车辆行为（如cut-in、急刹），增强策略鲁棒性 |
| **CTG（Cousin轨迹生成器）** | 解决数据偏差 | 对专家轨迹进行扩展与插值，生成更多“非直线”行为，构建 Cousin-nuScenes 数据集 |

#### 🏗️ **两阶段训练流程**

| 阶段 | 内容 |
|---|---|
| **阶段1：模仿学习** | 使用 CTG+DAA 生成的数据训练初始策略（行为克隆） |
| **阶段2：强化学习** | 在 ReconSimulator 中闭环训练，DAA 实时生成新corner cases，策略通过PPO优化 |

#### 📊 **实验结果**

| 方法 | Collision Ratio ↓ | 说明 |
|---|---|---|
| VAD（模仿学习） | 0.386 | 缺乏corner cases，闭环表现差 |
| RAD（RL+3DGS） | 0.238 | 有RL但仍受限于渲染质量和数据分布 |
| **ReconDreamer-RL** | **0.077** | 相比模仿学习 **↓5×**，相比RAD **↓3×** |

> 在 cut-in、急刹、对向车道入侵等corner cases中，ReconDreamer-RL 显著优于其他方法。

#### **贡献总结**

1. **首次**将**视频扩散先验**引入**驾驶场景重建+强化学习**，显著缩小Sim2Real Gap。
2. 提出**DAA**与**CTG**，解决corner case缺失与数据分布偏差问题。
3. 在 nuScenes 和 Waymo 上均验证有效，**碰撞率降低5倍**，**渲染速度125 FPS**，支持高效RL训练。


> **ReconDreamer-RL 用扩散模型“重建+增强”真实驾驶场景，自动生成corner cases，让自动驾驶策略在仿真中也能“见过世面”，从而更安全、更鲁棒。**


## Diffusion

### VFP: Variational Flow-Matching Policy for Multi-Modal Robot Manipulation

> 没太懂...主要是太基础了，很数学，先放这里，之后再看。

这篇论文提出了 **VFP**（Variational Flow-Matching Policy），目标很明确：  
> 让**基于流匹配（flow-matching）的策略**也能像扩散模型一样，**建模机器人在复杂任务中“多模态”的动作分布**，同时保持流匹配原有的**超快推理速度**。

#### 🎯 研究背景

- **扩散策略（diffusion policy）** 能很好地建模“一个状态 → 多种合理动作”的多模态分布，但**采样慢**（需要20步去噪）。  
- **流匹配（flow matching）** 只需**一步ODE积分**，推理速度是扩散的5倍，但**天生会把多模态平均成单峰分布**，导致“动作模糊”甚至失败。

#### 🧩 核心贡献

| 模块 | 作用 | 技术亮点 |
|---|---|---|
| **变分潜变量 z（Variational Latent Prior）** | 为每个模式分配一个“开关” | 让流解码器**不再平均所有动作**，而是根据 z 生成对应模式 |
| **Kantorovich-Optimal Transport（K-OT）** | 显式对齐“专家分布 vs 预测分布” | 避免漏掉任何专家模式，**全局分布级匹配** |
| **专家混合解码器（MoE-Flow）** | 每个专家只学一个模式 | 低计算量、易并行，**推理时只激活一个专家** |

> “先用潜变量 z 选模式，再用对应专家做一步流匹配，最后用 OT 保证所有模式都被覆盖。”

#### 📊 实验结果

| 场景 | VFP vs 最强基线 | 说明 |
|---|---|---|
| **Franka Kitchen**（任务多模态） | +11.5% 成功率 | 避免“来回切换任务”的犹豫行为 |
| **D3IL Avoid**（路径多模态） | +61.7% 成功率 | 避免“直撞障碍物”的平均路径 |
| **Adroit & Meta-World**（大规模） | +4~15% 平均 | 在复杂手物操作任务上更鲁棒 |

- **推理速度**：比扩散模型快 **4.6×**，比 FlowPolicy 仅慢 **5.6%**。  
- **模型大小**：与基线相当，甚至略小。



## 其他

### AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation

> 卢策吾老师它们开发的具身用交互环境，码

> AgentWorld 是一个**面向家庭场景**的**全链路仿真 + 数据采集平台**：  
> 先用程序自动生成**可交互、可渲染、可物理模拟**的客厅/卧室/厨房，再用 VR/键盘双模式**远程操作轮式或人形机器人**收集大规模演示数据，最后给出**1000+ 条轨迹的 AgentWorld Dataset**，让模仿学习算法（BC、ACT、Diffusion Policy、π0）都能**在 sim 训练后几秒内 zero-/few-shot 迁移到真机**。

#### 🎯 研究痛点  
1. **现有仿真器**要么只做**场景生成**，要么只做**任务数据集**，缺少**端到端、移动+操作一体化**的流水线。  
2. **家庭环境**布局、材质、光照、物体摆放高度可变，需要**程序 + 人工可编辑**的灵活生成。  
3. **真机采集**耗时、危险；需要**高保真 VR 远程操作**来快速攒大规模演示。

#### 🧩 平台能力总览  

| 模块 | 子功能 | 技术要点 |
|---|---|---|
| **程序场景生成** | 4 步流水线 | ① 布局自动生成（墙/楼梯/多楼层）<br>② 语义资产库 >9000 件（家具、可交互物体）<br>③ PBR 材质随机配置（木/金属/陶瓷…）<br>④ Isaac Sim PhysX 5.0 自动物理绑定 |
| **移动遥操作** | 双模式 | • 键盘：轮式/人形底盘（vx, vy, vθ）<br>• VR：手部关键点 → 逆运动学 → 机械臂/五指手 |
| **数据集** | 1.15 k 轨迹 | 基础任务（抓放、开闭、推拉）<br>多阶段任务（客厅整理、卧室铺床、厨房热饭）<br>4 种机器人形态（G1、H1、Franka、X-Trainer） |
| **验证实验** | sim→real | π0 在 sim 预训练 + 3 条真机微调 → 29.3% 成功率 |

#### 📊 关键结果  

| 任务类别 | 最佳算法 | 成功率 | 备注 |
|---|---|---|---|
| **基础操纵** | ACT | 66–84% | 短序列，动作块机制有效 |
| **多阶段长任务** | π0 | 20–30% | 语言+视觉预训练带来**长程结构理解** |
| **sim→real 迁移** | π0 | 29.3% | 仅用 3 条真机演示即可收敛到可执行策略 |

#### 🔍 亮点与局限  

| 亮点 | 具体表现 |
|---|---|
| **一站式** | 场景生成 + VR 采集 + 数据集 + 迁移验证全链路打通 |
| **高保真** | Unreal 渲染 + PBR 材质 + Isaac Sim 物理，缩小视觉/动力学差距 |
| **可扩展** | 新资产仅需一次语义标注，后续完全自动 |

| 局限 | 未来方向 |
|---|---|
| 软体/布料尚未支持 | 引入可变形物体引擎 |
| 复杂长任务仍需真机微调 | 提升纯合成数据泛化能力 |

#### 🧭 一句话收尾  
AgentWorld 让研究者 **“一键生成客厅，戴上 VR 手柄，十分钟就能攒出 100 条高质量演示”**，为家庭级移动操作提供了**开箱即用的仿真-真机闭环工具链**。


### CBDES MoE: Hierarchically Decoupled Mixture-of-Experts for Functional Modules in Autonomous Driving

[arxiv](https://arxiv.org/abs/2508.07838) 暂未开源

> 感知模块的 MoE，很神秘

CBDES-MoE 把传统 BEV 感知里“一条固定 CNN/Transformer 走到黑”的思路，升级成**“多条异构主干并行，由轻量级路由器按需激活”** 的 Mixture-of-Experts 架构。

#### 🎯 痛点与动机  
1. 单一主干（ResNet/Swin/ConvNeXt/PVT）**无法通吃**所有天气、光照、视角变化。  
2. 现有动态卷积/可变形注意力只能在**微观层面调参**，宏观结构仍旧死板。  
3. MoE 在 NLP/通用视觉很火，但在**多模态 BEV 感知**里还没系统落地。

#### 🧩 方案拆解（3 个关键词：异构专家、轻路由、软融合）

| 模块                            | 作用                      | 设计细节                                                     |
| ------------------------------- | ------------------------- | ------------------------------------------------------------ |
| **4 个异构专家**                | 提供“风格”多样的视觉表征  | Swin-T、ResNet-50、ConvNeXt、PVT 各成一路，结构差异带来互补偏置 |
| **Self-Attention Router (SAR)** | 0.1 ms 内决定“选哪位专家” | 小卷积 + 单层 MHA + MLP，输出 4-way softmax                  |
| **Top-1 稀疏激活**              | 推理时只跑 1 个专家       | 训练用软加权（可导），推理用 top-1（快），算力从 K×→1×       |
| **软融合 & 负载均衡**           | 避免路由塌缩到一两个专家  | soft fusion 保证梯度稳定；额外 L_balance 让 4 专家使用率尽量平均 |
| **即插即用**                    | 不改 BEVFusion 其余流程   | 把原来的“相机 backbone”整块替换成 CBDES-MoE，后面 view-transform、LiDAR 融合、检测头全部复用 |

#### 📊 nuScenes 结果（3D 检测）

| 模型                  | mAP↑     | NDS↑     | 备注                                              |
| --------------------- | -------- | -------- | ------------------------------------------------- |
| BEVFusion-Swin        | 64.0     | 65.6     | 最强单主干                                        |
| BEVFusion-ResNet      | 63.3     | 65.2     | —                                                 |
| BEVFusion-ConvNeXt    | 61.6     | 65.2     | —                                                 |
| BEVFusion-PVT         | 62.4     | 65.7     | —                                                 |
| **CBDES-MoE (top-1)** | **65.6** | **69.8** | 只跑 1 个专家，仍比所有单主干高 1.6 mAP / 4.1 NDS |

- 训练阶段 4 专家全部更新；推理阶段每张图片只激活 1 个，**显存 & 延迟几乎与单主干持平**。  
- 消融显示：去掉负载均衡 → mAP 掉 2.2 点；把 4 专家换成同构 4×ResNet → 提升微弱，说明“异构”才是关键。


